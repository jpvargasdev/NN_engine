# NN_engine


# ğŸ§  Neural Network Cheat Sheet

---

## âš™ï¸ 1. Forward Pass

**Pre-activation (input to neuron):**
```
z^[l] = W^[l] Â· a^[l-1] + b^[l]
```

**Activation (output of neuron):**
```
a^[l] = Ïƒ(z^[l])
```

Where:
- `W^[l]`: weight matrix of layer l
- `b^[l]`: bias vector of layer l
- `a^[l-1]`: activations from previous layer
- `Ïƒ`: activation function (commonly sigmoid, tanh, ReLU, etc.)

---

## ğŸ’¥ 2. Loss Function (MSE)

**Mean Squared Error:**
```
L = (1/n) âˆ‘ (Å·áµ¢ - yáµ¢)Â²
```

**Gradient of Loss w.r.t. prediction (output):**
```
âˆ‚L/âˆ‚Å· = 2(Å· - y)
```

---

## ğŸ” 3. Backpropagation

**Output layer error:**
```
Î´^[L] = (Å· - y) âŠ™ Ïƒ'(z^[L])
```

**Hidden layer error (general case):**
```
Î´^[l] = (W^[l+1])áµ— Â· Î´^[l+1] âŠ™ Ïƒ'(z^[l])
```

Where:
- `Î´^[l]`: error in layer l
- `âŠ™`: element-wise multiplication (Hadamard product)
- `Ïƒ'(z)`: derivative of activation function

For sigmoid:
```
Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z)) = a^[l](1 - a^[l])
```

---

## ğŸ“‰ 4. Gradient Descent Updates

**Update rule for weights:**
```
W^[l] := W^[l] - Î· âˆ‚L/âˆ‚W^[l]
```

**Update rule for biases:**
```
b^[l] := b^[l] - Î· âˆ‚L/âˆ‚b^[l]
```

Where:
- `Î·`: learning rate

**Gradients:**
```
âˆ‚L/âˆ‚W^[l] = Î´^[l] Â· (a^[l-1])áµ—
âˆ‚L/âˆ‚b^[l] = Î´^[l]
```

---

## ğŸ”¢ Dimensions Recap

- `W^[l]`: shape = [neurons in layer l, neurons in layer l-1]
- `b^[l]`: shape = [neurons in layer l, 1]
- `a^[l]`, `Î´^[l]`: shape = [neurons in layer l, 1]

